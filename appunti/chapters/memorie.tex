Esiste una gerarchia di memoria, partono da memorie molto veloci come i registri, le cache arriviamo poi alla main memory e alla disk memory, sempre piu' lente e sempre piu' grandi.

Ci sono i principi di localita' e gerarchia di memorie. Il principio di localita' spaziale afferma che i programmi tendono ad accedere a una porzione ristretta di memoria in un dato momento, se accedo ad i e' molto probabile che acceda ad un indirizzo li' vicino (delta i), il che rende vantaggioso mantenere i dati frequentemente utilizzati nelle memorie piu' veloci. La gerarchia di memorie sfrutta questo principio organizzando le memorie in livelli, con memorie piu' veloci e costose in alto e memorie piu' lente e economiche in basso.
Mentre la localita' temporale si riferisce alla tendenza ad eseguire la stessa istruzione piu' volte in un breve intervallo di tempo.

Il modello di memoria CUDA, abbiamo visto che esistono delle memorie programmabili che hanno pattern di accesso utilizzabili con le API di sistema e devono essere gestite direttamente, devono essere gestite in maniera diversa per la lettura e la scrittura. Poi ci sono delle memorie non programmabili, le cache L1 e L2.

\section{Registri}
Sono le memorie piu' veloci e piu' piccole. Sono ripartiti tra i warp attivi (assegnati ad ogni thread). Le variabili locali sono quelle che vengono assegante ai registri senza che sia specificato nessuno dei qualificatori generalmente risiede in un registro.
Meno registri usa il kernel, piu' blocchi di thread e' probabile che risiedano sul multiprocessore, quindi meno ne usiamo meglio e'.

\section{Local Memory}
E' una memoria lenta come la global memory. E' una memoria locale ai thread ed e' usata per contenere le variabili automatiche (grandi) non contenute nei registri:
\begin{itemize}
    \item Array locali i cui indici non possono essere determinati a compile-trasferimento
    \item Strutture locali grandi che consumano troppo spazio registro
    \item Ogni variabile che non puo' essere allocata nei registri a causa numero limitato (register spilling)
\end{itemize}
Risiede nella device memory, pertanto gli accessi hanno stessa latenza e ampiezza di banda della global memory e sono soggetti anche agli stessi vincoli di di coalescenza.
Il compilatore nvcc si preoccupa della sua allocazione e non e' controllata dal programmatore.

\section{Constant Memory}
Risiede in device memory ed ha una cache dedicata in ogni SM. E' ideale per ospitare dati a cui si accede in modo uniforme e in sola lettura. 
Una variabile dichiarata come \texttt{\_\_constant\_\_} viene dichiarata con scope globale, ad di fuori di qualsiasi kernel. 
L'utilizzo migliore come dice il nome e' allocarci dati che devono essere distribuiti tra tanti kernel.

\begin{lstlisting}
    cudaError_t cudaMemcpyToSymbol(const void *symbol, const void *src, size_t count);
\end{lstlisting}

E' pari a 64KB per tutte le compute capability.

\section{Texture Memory}
E' una memoria in sola lettura, dotata di cache e risiede in device memory. E' particolarmente utile perche' include un supporto hw efficiente per filtraggio o interpolazione floating-point nel processo di lettura dei dati. E' ottimizzata per la localita' spaziale 2D, quindi dati espressi sotto forma di matrici.
I thread di un warp che usano la texture memory per accedere a dati 2D hanno migliori prestazioni rispetto a quelle standard.

\section{Shared Memory}
E' una memoria programmabile on-chip con un bandwidth molto piu' alto e minor latenza della local o global memory. E' suddivisa in moduli della stessa ampizza, chiamati bank che possono essere acceduti da un thread alla volta.
Abbiamo 32 banchi di memoria che devono essere utilizzati in maniera opportuna per essere acceduti simultaneamente.
Ogni SM ha una quantita' limitata di shared memory che viene ripartita tra i blocchi di thread. 
Bisogna farne un uso parco ancora una volta per non limitare il numero di warp attivi.
Con i blocchi di thread condivide anche lifetime e scope.
E' fondamentale per la comunicazione inter-thread di un blocco, poiche' permette di scambiare dati in modo rapido e con bassa latenza.
L'accesso deve essere sincronizzato per mezzo delle seguente CUDA runtime call:
\begin{lstlisting}
    __syncthreads();
\end{lstlisting}

%metti immagine

\subsection{Organizzazione}
La SMEM dell'arch Fermi sono suddivisi in blocchi da 4 byte (chiamate word) da 32 bit (0 64). Ogni word puo' contenere tipi base (1 int, 1 float, 2 short, 4 char).
la bandwitdth e' di 32 bit ogni 2 cicli di clock.
Nei 32 banchi andiamo ad accedere simultaneamente a 32 word, ed e' l'accesso piu' veloce che possiamo avere:
%immagine organizzazione

\subsection{SMEM runtime}
Viene ripartita tra tutti i blocchi residenti in un SM. Maggiore e' la shared memory richiesta da un kernel, minore e' il numero di blocchi attivi concorrenti. Il contenuto della shared memory ha lo stesso lifetime del blocco cui e' stata assegnata. L'accesso e' per warp: caso migliore 1 transazione x 32 thread, caso peggiore 32 transazioni.

\subsection{Pattern di accesso}
Se un'operazione di load o store eseguita da un warp richiede al piu' un accesso per bank, si puo' effettuare in una sola transizione il traferimento dati dalla shared memory al warp.

%esempio

\begin{itemize}
    \item Caso broadcast: un singolo valore letto da tutti i thread in un warp da un singolo bank
    \item Caso parallelo: un singolo valore letto da un singolo bank. In questo caso le cose vanno bene, per ogni thread accediamo al corrispondente banco di memoria.
    \item Conflitto doppio: tutti i thread di un warp richiedono una word di indice doppio il threadId.x
    \item Nessun conflitto: strutture di 3 word come, ad esempio, le terne di punti nello spazio.
\end{itemize}

\subsection{Conflitto di accesso}
Il bank conflit e' quando diversi indirizzi di shared memory insistono sullo stesso bank. L'hw effettua tante transizioni quante ne sono necessarie per risolvere il conflitto, riducendo le prestazioni.

%esempio

\subsection{Configurazione SMEM / cache L1}
Ogni SM ha 64 Kb di memoria on-chip, la sherd memory e L1 cache condividono questa risorsa HW. 
Cuda fornisce due metodi per configurare la L1 cache e la shared memory:
\begin{lstlisting}
   cudaError_t cudaDeviceSetCacheConfig(cudaFuncCache cacheConfig);
\end{lstlisting}

Dove l'argomento cacheConfig specifica come deve essere ripartita la memoria:
\begin{itemize}
    \item cudaFuncCachePreferNone: no preference
    \item cudaFuncCachePreferShared: prefer shared memory 48KB shared e 16 Kb L1
    \item cudaFuncCachePreferL1: prefer L1 cache
    \item cudaFuncCachePreferEqual: equal preference
\end{itemize}

\subsection{Osservazioni}
Il latency hiding e' sempre una cosa indesiderata, il ritardo che puo' incorrere alla SMEM e all'ottenimento dei dati non e' in generale un problema anche in caso di conflitti di banchi, perche' se si riesce a tenere alto il numero di warp attivi, thread in esecuzione e blocchi letti in esecuzione si ha sempre un sostituto nel momento in cui un warp va in attesa.
Inter-block conflict, non esiste un conflitto tra thread appartenenti a blocchi differenti, il problema sussite solo a livello di warp dello stesso blocco.
Il modo piu' semplice per avere prestazioni elevate e' quello di fare in modo che un warp acceda a word consecutive in memoria shared.
Caching: con lo scheduling efficace le prestazioni anche in presenza di conflitti a livello di SMEM sono di gran lunga migliori se paragonate a quelle in cui il cammino che i dati percorrono passa attraverso la cache L2 o peggio, arriva in global memory.

\subsection{Allocazione statica delle SMEM}
Una variabile in shared memory puo' anche essere dichiarata sia locale a un kernel sia globale in un file sorgente.
E' dichiarata con il qualificatore \texttt{\_\_shared\_\_}.
Puo' essere dichiarata sia staticamente sia dinamicamente.

\subsection{Allocazione dinamica della SMEM}
Se la dimensione non e' nota a tempo di compilazione e' possibile dichiarare una variabile adimensionale con la keyword extern:
\begin{itemize}
    \item Dinamicamente si possono allocare solo array 1D
    \item puo' essere sia internal al kernel sia esterna
\end{itemize}

\subsection{Uso tipico}
\begin{itemize}
    \item Caricare i dati dalla device memory alla shared memory
    \item Sincronizzare i thread del blocco al termine della copia che ognuno effettua sulla shared memory (cosi' che ogni thread possa elaborare dati certi nel prosieguo)
    \item Elabora i dati in shared memory
    \item Sincronizza per essere certi che la shared memory contenga i risultati aggiornati
    \item Scrivi i risultati dalla device memory alla host memory
\end{itemize}

\subsection{Prodotto matriciale con SMEM}
Nella versione senza memoria shared ogni thread e' responsabile del calcolo di un elemento della matrice prodotto. Dobbiamo tenere conto che abbiamo una suddivisione logica dei thread che stanno in un blocco e degli elementi in memoria corrispondenti.
Passi con shared memory:
\begin{itemize}
    \item Occorre caricare in memoria shared i dati relativi ad ogni blocco prima di svolgere le somme e i prodotti
    \item Ogni thread accumula i risultati di ognuno di questi prodotti (scandendo tutti i blocchi) in un registro
    \item Alla fine scrive su global memory
\end{itemize}

Qui l'idea e' di trasferire gli elementi di riga e colonna delle due matrici in shared memory perche' dobbiamo accedere ad ogni elemento tante volte e quindi l'accesso a quella memoria sarebbe piu' veloce.


L'idea e':
\begin{itemize}
    \item Caricare il primo tile 
    \item Effettuare le somme parziali per ogni cella
    \item Riptere per il numero di tile contenuti nelle zone delle matrici da caricare
    \item Scrivere i risultati dalla shared memory alla global memory
\end{itemize}

\subsection{Prodotto convolutivo con SMEM}
La convoluzione e' un prodotto tra due vettori, tendenzialmente un vettore piu' grande ed uno piu' piccolo chiamato kernel o maschera.

Una volta che abbiamo la scrittura matematica l'istanziazione e' con vettori, nell'esempio vediamo il segnale e la maschera, viene evidenziato il segnale centrale perche' e' quello in output della convoluzione:
%metti immagine implementazione
Occorre fare shiftare la maschera con uno stride = 1 e calcolare prodotti e somme.

Da notare che ci sono casi particolari, nell'esempio partiamo dal terzo elemento perche' c'e' corrispondenza reale con la maschera, si potrebbe anche adottare approcci diversi relativamente a dati mancanti.

Questa cosa si estende naturalmente a piu' dimensioni, questo e' l'esempio del 2D:
%esempio immagini 2D

Ha senso pensare all'uso della shared memory perche' alcune celle della matrice di input vengono riutilizzate più volte durante il calcolo della convoluzione (tutte quelle sotto la maschera sicuramente).

Questo e' un problema ideale per il parallel computing:
\begin{itemize}
    \item Determinare la mappa tra thread ed elementi di output
    \item Semplice approccio nei casi 1D e 2D e' organizzare i thread in grid corrispondenti in modo tale che ogni thread calcoli un elemento della matrice di convoluzione 
    \item Problema: non soddisfa per inefficienza negli accessi alla global memory
\end{itemize}

%esempio convoluzione parallela

\subsubsection{Tiling}
Divido i dati in blocchi (ad esempio, 16 elementi in 4 blocchi da 4 thread), ogni thread nel blocco fa un prodotto della convoluzione. Per ridurre l'accesso alla global memory, in cache/memoria condivisa si tengono i dati a cui l'accesso è fatto più frequentemente, ovvero i valori del "blocco di dati" assegnato al block (tutti i thread devono calcolare sullo stesso insieme di dati, o quasi), tenendo conto della dimensione della maschera (serve avere i dati "adiacenti" al blocco, quelli che sbordano (sarebbe molto utile un'immagine, si capirebbe subito)).

I dati da caricare in smem sono più dei thread nel blocco ("alone" che va al di fuori del blocco di dati stesso); in smem carico tutti i possibili dati a cui il blocco deve fare l'accesso. 

Chi carica che dati in memoria? I dati esterni al blocco potrebbero essere anche più del blocco stesso (maschera "grossa"). La soluzione è dare un ordine ai thread e dividere il più equamente possibile i caricamenti in memoria tra i thread del blocco. Si può fare facendo un tiling dei dati da caricare: il blocco viene "ripetuto" sui dati da caricare, secondo un ordine dei thread.

\section{Global Memory}

Nei computer moderni esiste una gerarchia di memorie per minimizzare latenze e massimizzare throughput. In genere, si ha l'illusione virtuale di una grande memoria, tutta a bassa latenza, anche se la memoria con effettivamente bassa latenza è poca e si ha una memoria ad alta capacità e alta latenza.

All'interno delle GPU abbiamo, dalla latenza più alta alla più bassa: 
\begin{itemize}
	\item Device Memory
	
	\item L2 Cache
	
	\item L1/shared
	
	\item Registers
\end{itemize}

Le gerarchie di memorie, comprese quelle CUDA, fanno fede ai principi di: 
\begin{itemize}
	\item \textbf{Località spaziale}: se l'istruzione all'indirizzo $i$ viene eseguita, probabilmente dopo verrà eseguita quella all'indirizzo $i + \Delta i$
	
	\item \textbf{Località temporale}: se un'istruzione viene eseguita al tempo $t$, probabilmente verrà eseguita anche al tempo $t + \Delta t$ (dove $\Delta t$ è piccolo)
\end{itemize}

\paragraph{Registers:} Le memorie più veloci in assoluto, con lifetime del kernel. Vengono ripartiti tra i warp attivi, le variabili dichiarate nel codice device senza qualificatori generalmente risiedono in un registro. 

Meno registri usa il kernel, più blocchi di thread è probabile che risiedano sull'SM (il compilatore usa un'euristica per ottimizzare questo parametro). \textbf{Register spilling}: se si usano più registri di quelli consentiti le variabili si riversano nella local memory.

\paragraph{Local Memory:} Si tratta di una memoria \textit{lenta} (collocata off-chip, alta latenza, bassa bandwidth). Si tratta di una memoria locale ai thread.

Usata per contenere le variabili automatiche (grandi) non contenute nei registri, o per le variabili al di fuori causa spilling. La local memory risiede nella device memory, pertanto gli accessi hanno stessa latenza e ampiezza di banda della global memory e sono soggetti anche agli stessi vincoli di coalescenza.

Da CC 2.0 ci sono parti poste in cache L1 a livello di SM e in cache L2 a livello di device. Il compilatore \texttt{nvcc} si preoccupa della sua allocazione e non è controllata dal programmatore.

\paragraph{Constant Memory:} Risiede nella device memory (64K per tutte le CC) ed ha una cache dedicata in ogni SM (8K). Definibile tramite l'attributo \texttt{\_\_constant\_\_}. 

Ospita dati in sola lettura, ideale per accessi uniformi. Ha scope globale, va dichiarata al di fuori di qualsiasi kernel e viene dichiarata staticamente, quindi è visibile a tutti i kernel nella stessa unità di compilazione.

La constant memory può essere inizializzata dall'host usando:
\begin{lstlisting}
cudaError_t cudaMemcpyToSymbol(const void* symbol,
    const void* src, size_t count)
\end{lstlisting}

Lavora bene quando tutti i thread di un warp leggono dallo stesso indirizzo di memoria (raggiunge l'efficienza dei registri); se i thread di un warp leggono da indirizzi diversi allora le letture vengono serializzate, riducendo l'efficienza.

\paragraph{Texture Memory:} Risiede nella device memory e (può avere) una read-only cache per-SM ed è acceduta solo attraverso di essa. La cache include un supporto hardware efficiente per filtraggio o interpolazione floating-point nel processo di lettura dei dati. 

Ottimizzata per la località spaziale 2D, quindi dati espressi sotto forma di matrici. I thread in un warp che usano la texture memory per accedere a dati 2D hanno migliori prestazioni rispetto a quelle standard, quindi è adatta per applicazioni in cui servono classiche elaborazioni di immagini/video. Per altre applicazioni l’uso della texture memory potrebbe essere più lento della global memory.

\paragraph{Global Memory:} La più grande, con più alta latenza e più comunemente usata memoria su GPU. Ha scope e lifetime globale (da qui il nome). Dichiarazione (codice host):
\begin{center}
	\begin{tabular}{r | l r | }
		\cline{2-3}
		\textbf{Statica} & \texttt{\_\_device\_\_ int a[N];} & \\
		\textbf{Dinamica} & \texttt{cudaMalloc((void **)\&d\_a, N);} & \texttt{cudaFree(d\_a);} \\
		\cline{2-3}
	\end{tabular}
\end{center}

Corrisponde alla memoria fisica, con "global" si intende una divisione logica. L'accesso da parte di thread appartenenti a blocchi distinti può potenzialmente portare a modifiche incoerenti. La global memory è accessibile attraverso transazioni da 32, 64, o 128 byte; le transazioni avvengono solo per gruppi di valori, non si può accedere a un valore singolo.

I valori contenuti nella memoria allocata non sono inizializzati, ma si possono inizializzare con dati provenienti dall'host (\texttt{cudaMemcpy()}) oppure con un valore specifico
\begin{lstlisting}
cudaError_t cudaMemset(void* devPtr, int value, size_t count)
\end{lstlisting}

Assegna il valore \texttt{value} a tutti gli indirizzi contenuti nel blocco di memoria.

La memoria allocata e' opportunamente allineata per ogni tipo di variabile. La \texttt{cudaMalloc()} restituisce \texttt{cudaErrorMemoryAllocation} in caso di fallimento.

Lo \textbf{specificatore \texttt{\_\_device\_\_}} indica una variabile che risiede unicamente sul device. Risiede nella memoria globale (e quindi oggetti distinti per device diversi), ha il lifetime del contesto CUDA in cui è stata creata. Può essere acceduta da tutti i thread e dall'host tramite la libreria runtime:
\begin{itemize}
	\item \texttt{cudaGetSymbolAddress()}, \texttt{cudaGetSymbolSize()}: per ottenere indirizzo e dimensione di una variabile, rispettivamente
	
    \item \texttt{cudaMemcpyToSymbol()}, \texttt{cudaMemcpyFromSymbol()}: per copiare verso e da una variabile, rispettivamente
\end{itemize}

Riassunto dichiarazione di variabili: 
\begin{center}
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		\textbf{QUALIFIER} & \textbf{VARIABLE} & \textbf{MEMORY} & \textbf{SCOPE} & \textbf{LIFESPAN} \\
		\hline
		& \texttt{float var} & Register & Local & Thread \\
		\hline
		& \texttt{float var[100]} & Local & Local & Thread \\
		\hline
		\texttt{\_\_shared\_\_} & \texttt{float var} & Shared & Block & Block \\
		\hline
		\texttt{\_\_device\_\_} & \texttt{float var} & Global & Global & Application \\
		\hline
		\texttt{\_\_constant\_\_} & \texttt{float var} & Constant & Global & Application \\
		\hline
	\end{tabular}
\end{center}

\subsection{Pinned memory}
La pinned memory (o page-locked memory) in CUDA è una tecnica che serve per ottimizzare il trasferimento dei dati tra la memoria del sistema (RAM) e la memoria della GPU (VRAM). 

Si vuole evitare il page fault della virtual memory (CPU, di default la memoria host allocata è paginabile). Esistono delle primitive per definire una memoria pinned, ovvero viene tolta la pagina dal meccanismo di virtualizzazione in modo che l'host non possa "toglierla" mentre il device la deve usare. Blocca la memoria in modo da poter fare trasferimenti asincroni al device.

Una volta "pinnata", la memoria non sparirà dal sistema di virtualizzazione automatico della memoria host, quindi si può lavorare su quella memoria in maniera asincrona. La pinned memory può essere acceduta direttamente dal device, in modalità asincrona. Può essere letta e scritta con una bandwidth più alta rispetto alla memoria paginabile.

Da notare che eccessi di allocazione di pinned memory potrebbero far degradare le prestazioni dell'host (ridurre la memoria paginabile inficia l'uso della virtual memory), Per allocare esplicitamente memoria pinned:
\begin{lstlisting}
cudaError_t cudaMallocHost(void **devPtr, size_t count);
\end{lstlisting}

E per deallocarla:
\begin{lstlisting}
cudaError_t cudaFreeHost(void *devPtr);
\end{lstlisting}

Questa allocazione sostituisce la malloc "normale", su host. Rende i trasferimenti host-device significativamente più veloci, al costo di un tempo più alto di allocazione.

\subsection{Unified Virtual Addressing UVA}

Si vuole avere un unico spazio di indirizzamento tra CPU e tutte le GPU. Tutti i puntatori (CPU e GPU) appartengono allo stesso spazio di indirizzi virtuali, di conseguenza è possibile passare un puntatore da host a device e viceversa senza ambiguità, entrambi possono "capire" a cosa punta quell'indirizzo.

% End L5

La unified memory è una memoria "\textit{comoda}", fornisce un puntatore unico per tutte le CPU e GPU presenti nel sistema. Spazio di indirizzamento unico per CPU e GPU.

Con "\textbf{Managed Memory}" si fa riferimento ad allocazioni della unified memory. All'interno di un kernel si possono usare entrambi i tipi di memoria: 
\begin{itemize}
	\item managed memory, controllata dal sistema
    
	\item un-managed memory, controllata esplicitamente dall'applicazione
\end{itemize}

Tutte le operazioni CUDA valide sulla memoria del dispositivo sono valide anche sulla memoria managed.

Per fare allocazione dinamica:
\begin{lstlisting}

cudaError_t cudaMallocManaged(void **devPtr, size_t size, 
    unsigned int flags=0)
\end{lstlisting}

"rimpiazza" \texttt{cudaMalloc}, la \texttt{flag} indica chi condivide il puntatore con il device:
\begin{itemize}
	\item \texttt{cudaMemAttachHost}: solo la CPU
    
	\item \texttt{cudaMemAttachGlobal}: anche tutte le altre GPU
\end{itemize}

Nuova \textbf{keyword}: \textbf{\texttt{\_\_managed\_\_}}, si tratta di un qualifier che denota scope globale, accessibile da CPU e GPU.

Con l'uso "misto" di memoria bisogna porre attenzione alla sincronizzazione tra CPU e GPU, onde evitare problemi.

\subsection{Pattern di Accesso alla Global Memory}

Gli accessi alla memoria del dispositivo possono avvenire in transazioni da 32, 64 o 128 byte. Le applicazioni GPU tendono (a volte) ad essere limitate dalla memory bandwidth, quindi massimizzare il throughput effettivo è importante. 

In generale, per rendere efficienti le transazioni in memoria:
\begin{itemize}
	\item minimizzare il numero di transazioni per servire il massimo numero di accessi
    
	\item considerare che il numero di transazioni e throughput ottenuto variano con la CC
\end{itemize}

Per migliorare le prestazioni in lettura e scrittura occorre ricordare che: 
\begin{itemize}
	\item le istruzioni vengono eseguite a livello di warp e gli accessi in memoria dipendono dalle operazioni svolte nel warp
    
	\item per un dato indirizzo si esegue un'operazione di loading o storing (gestione diversa)
	
    \item i 32 thread presentano una singola richiesta di accesso, che viene servita da una o più transazioni in memoria
\end{itemize}

In base a come sono distribuiti gli indirizzi di memoria, gli accessi alla stessa possono essere classificati in pattern distinti. Tutti gli accessi a memoria globale passano dalla cache L2, molti passano anche dalla L1. Se entrambe le memorie vengono usate gli accessi sono da 128 byte, altrimenti, se viene usata solo la L2, gli accessi sono a 32 byte.

Per le architetture che usano cache L1, queste possono essere esplicitamente abilitate o disabilitate a compile time.

Bisogna rispettare allineamento e coalescenza per sfruttare al meglio le transazioni di memoria; per avere accessi in memoria efficienti è necessario combinare in un unica transazione accessi multipli a memoria allineati e coalescenti.

Accesso \textbf{allineato}: quando il primo indirizzo della transazione è un multiplo pari della granularità della cache che viene usata per servire la transazione (32 byte per la cache L2 o 128 byte per la cache L1).

Accesso \textbf{coalescente:} quando tutti i 32 thread in un warp accedono a un blocco contiguo di memoria.

In un SM i dati seguono pipeline attraverso i seguenti tre cache/buffer paths dipendentemente da quale tipo di device memory si accede:
\begin{itemize}
	\item L1/L2 cache
    
	\item Constant cache
	
    \item Read-only cache
\end{itemize}

L1/L2 cache rappresenta il default path. Il fatto che un'operazione di \texttt{load} in global memory passi attraverso la cache L1 dipende da CC e compiler options.

\paragraph{Scritture:} Le write vengono servite in modo diverso, non viene usata la cache L1, ma le \texttt{store} sono cachate solo in L2, prima di essere inviate alla device memory in segmenti da 32 byte; vengono trasferiti 1,2 o 4 segmenti alla volta.

Quando forzati a fare accessi (letture/scritture) non coalescenti si può usare la shared memory come "passaggio" per rendere le operazioni effettive in memoria coalescenti.

\paragraph{AoS vs SoA:} I dati possono essere divisi in: 
\begin{itemize}
	\item \textbf{Array of Structures AoS}:
	\begin{lstlisting}
struct Particle { float x, y, z; };
Particle* P;
float x = P[idx].x;  // stride = sizeof(Particle)
	\end{lstlisting}
	Questo porta a distanza tra accessi (stride) alta, rompendo la coalescenza
	
	\item \textbf{Structure of Arrays SoA}:
	\begin{lstlisting}
float *Px, *Py, *Pz;
float x = Px[idx];   // stride = 1
	\end{lstlisting}
	In questo modo lo stride è ridotto, riducendo così il numero di transazioni necessarie
\end{itemize}

\paragraph{TL;DR:} Un warp può effettuare accessi
\begin{itemize}
	\item coalescenti: i 32 thread leggono dati contigui, massima efficienza
	\item non coalescenti/strided: dati a distanza $>1$, possono servire più transazioni per la stessa quantità di dati, fino a 32 diverse
\end{itemize}

In generale (per CC superiori a 2) le transazioni coprono 128 byte. All'interno di un singolo segmento da 128 byte, la memoria è organizzata in "banks" (4 da 32 byte solitamente), anche se un thread legge solo 4 byte, dovrà trasferire l'intero bank.

La cache L1 serve load/store anche con granularità a 32 byte.

\subsubsection{Matrice trasposta con SMEM}

Il problema del parallelizzare l'operazione di trasposizione di una matrice  è che in lettura gli accessi sono per riga, quindi coalescenti, mentre le scritture sulla matrice trasposta sono per colonna, quindi non coalescenti.

Un'idea per risolvere il problema può essere: 
\begin{itemize}
	\item Caricare dalla global memory alla smem le celle che il blocco corrente deve trasporre, riga per riga
	
	\item Leggere una colonna in smem e scrivere una riga in global memory
\end{itemize}

Esempio:
\begin{lstlisting}
__shared__ float tile[BDIMY][BDIMX];
// Coordinate originali
int y = blockIdx.y * blockDim.y + threadIdx.y;
int x = blockIdx.x * blockDim.x + threadIdx.x;

// ...
// Trasferimento in smem e sincronizzazione

// Nuovi indici
int y = blockIdx.x * blockDim.x + threadIdx.y;
int x = blockIdx.y * blockDim.y + threadIdx.x;

// ...
// Scrivere sulla matrice output, 
//      controlli invertiti tra riga e colonna
\end{lstlisting}