Esiste una gerarchia di memoria, partono da memorie molto veloci come i registri, le cache arriviamo poi alla main memory e alla disk memory, sempre piu' lente e sempre piu' grandi.

Ci sono i principi di localita' e gerarchia di memorie. Il principio di localita' spaziale afferma che i programmi tendono ad accedere a una porzione ristretta di memoria in un dato momento, se accedo ad i e' molto probabile che acceda ad un indirizzo li' vicino (delta i), il che rende vantaggioso mantenere i dati frequentemente utilizzati nelle memorie piu' veloci. La gerarchia di memorie sfrutta questo principio organizzando le memorie in livelli, con memorie piu' veloci e costose in alto e memorie piu' lente e economiche in basso.
Mentre la localita' temporale si riferisce alla tendenza ad eseguire la stessa istruzione piu' volte in un breve intervallo di tempo.

Il modello di memoria CUDA, abbiamo visto che esistono delle memorie programmabili che hanno pattern di accesso utilizzabili con le API di sistema e devono essere gestite direttamente, devono essere gestite in maniera diversa per la lettura e la scrittura. Poi ci sono delle memorie non programmabili, le cache L1 e L2.

\section{Registri}
Sono le memorie piu' veloci e piu' piccole. Sono ripartiti tra i warp attivi (assegnati ad ogni thread). Le variabili locali sono quelle che vengono assegante ai registri senza che sia specificato nessuno dei qualificatori generalmente risiede in un registro.
Meno registri usa il kernel, piu' blocchi di thread e' probabile che risiedano sul multiprocessore, quindi meno ne usiamo meglio e'.

\section{Local Memory}
E' una memoria lenta come la global memory. E' una memoria locale ai thread ed e' usata per contenere le variabili automatiche (grandi) non contenute nei registri:
\begin{itemize}
    \item Array locali i cui indici non possono essere determinati a compile-trasferimento
    \item Strutture locali grandi che consumano troppo spazio registro
    \item Ogni variabile che non puo' essere allocata nei registri a causa numero limitato (register spilling)
\end{itemize}
Risiede nella device memory, pertanto gli accessi hanno stessa latenza e ampiezza di banda della global memory e sono soggetti anche agli stessi vincoli di di coalescenza.
Il compilatore nvcc si preoccupa della sua allocazione e non e' controllata dal programmatore.

\section{Constant Memory}
Risiede in device memory ed ha una cache dedicata in ogni SM. E' ideale per ospitare dati a cui si accede in modo uniforme e in sola lettura. 
Una variabile dichiarata come \texttt{\_\_constant\_\_} viene dichiarata con scope globale, ad di fuori di qualsiasi kernel. 
L'utilizzo migliore come dice il nome e' allocarci dati che devono essere distribuiti tra tanti kernel.

\begin{lstlisting}
    cudaError_t cudaMemcpyToSymbol(const void *symbol, const void *src, size_t count);
\end{lstlisting}

E' pari a 64KB per tutte le compute capability.

\section{Texture Memory}
E' una memoria in sola lettura, dotata di cache e risiede in device memory. E' particolarmente utile perche' include un supporto hw efficiente per filtraggio o interpolazione floating-point nel processo di lettura dei dati. E' ottimizzata per la localita' spaziale 2D, quindi dati espressi sotto forma di matrici.
I thread di un warp che usano la texture memory per accedere a dati 2D hanno migliori prestazioni rispetto a quelle standard.

\section{Shared Memory}
E' una memoria programmabile on-chip con un bandwidth molto piu' alto e minor latenza della local o global memory. E' suddivisa in moduli della stessa ampizza, chiamati bank che possono essere acceduti da un thread alla volta.
Abbiamo 32 banchi di memoria che devono essere utilizzati in maniera opportuna per essere acceduti simultaneamente.
Ogni SM ha una quantita' limitata di shared memory che viene ripartita tra i blocchi di thread. 
Bisogna farne un uso parco ancora una volta per non limitare il numero di warp attivi.
Con i blocchi di thread condivide anche lifetime e scope.
E' fondamentale per la comunicazione inter-thread di un blocco, poiche' permette di scambiare dati in modo rapido e con bassa latenza.
L'accesso deve essere sincronizzato per mezzo delle seguente CUDA runtime call:
\begin{lstlisting}
    __syncthreads();
\end{lstlisting}

%metti immagine

\subsection{Organizzazione}
La SMEM dell'arch Fermi sono suddivisi in blocchi da 4 byte (chiamate word) da 32 bit (0 64). Ogni word puo' contenere tipi base (1 int, 1 float, 2 short, 4 char).
la bandwitdth e' di 32 bit ogni 2 cicli di clock.
Nei 32 banchi andiamo ad accedere simultaneamente a 32 word, ed e' l'accesso piu' veloce che possiamo avere:
%immagine organizzazione

\subsection{SMEM runtime}
Viene ripartita tra tutti i blocchi residenti in un SM. Maggiore e' la shared memory richiesta da un kernel, minore e' il numero di blocchi attivi concorrenti. Il contenuto della shared memory ha lo stesso lifetime del blocco cui e' stata assegnata. L'accesso e' per warp: caso migliore 1 transazione x 32 thread, caso peggiore 32 transazioni.

\subsection{Pattern di accesso}
Se un'operazione di load o store eseguita da un warp richiede al piu' un accesso per bank, si puo' effettuare in una sola transizione il traferimento dati dalla shared memory al warp.

%esempio

\begin{itemize}
    \item Caso broadcast: un singolo valore letto da tutti i thread in un warp da un singolo bank
    \item Caso parallelo: un singolo valore letto da un singolo bank. In questo caso le cose vanno bene, per ogni thread accediamo al corrispondente banco di memoria.
    \item Conflitto doppio: tutti i thread di un warp richiedono una word di indice doppio il threadId.x
    \item Nessun conflitto: strutture di 3 word come, ad esempio, le terne di punti nello spazio.
\end{itemize}

\subsection{Conflitto di accesso}
Il bank conflit e' quando diversi indirizzi di shared memory insistono sullo stesso bank. L'hw effettua tante transizioni quante ne sono necessarie per risolvere il conflitto, riducendo le prestazioni.

%esempio

\subsection{Configurazione SMEM / cache L1}
Ogni SM ha 64 Kb di memoria on-chip, la sherd memory e L1 cache condividono questa risorsa HW. 
Cuda fornisce due metodi per configurare la L1 cache e la shared memory:
\begin{lstlisting}
   cudaError_t cudaDeviceSetCacheConfig(cudaFuncCache cacheConfig);
\end{lstlisting}

Dove l'argomento cacheConfig specifica come deve essere ripartita la memoria:
\begin{itemize}
    \item cudaFuncCachePreferNone: no preference
    \item cudaFuncCachePreferShared: prefer shared memory 48KB shared e 16 Kb L1
    \item cudaFuncCachePreferL1: prefer L1 cache
    \item cudaFuncCachePreferEqual: equal preference
\end{itemize}

\subsection{Osservazioni}
Il latency hiding e' sempre una cosa indesiderata, il ritardo che puo' incorrere alla SMEM e all'ottenimento dei dati non e' in generale un problema anche in caso di conflitti di banchi, perche' se si riesce a tenere alto il numero di warp attivi, thread in esecuzione e blocchi letti in esecuzione si ha sempre un sostituto nel momento in cui un warp va in attesa.
Inter-block conflict, non esiste un conflitto tra thread appartenenti a blocchi differenti, il problema sussite solo a livello di warp dello stesso blocco.
Il modo piu' semplice per avere prestazioni elevate e' quello di fare in modo che un warp acceda a word consecutive in memoria shared.
Caching: con lo scheduling efficace le prestazioni anche in presenza di conflitti a livello di SMEM sono di gran lunga migliori se paragonate a quelle in cui il cammino che i dati percorrono passa attraverso la cache L2 o peggio, arriva in global memory.

\subsection{Allocazione statica delle SMEM}
Una variabile in shared memory puo' anche essere dichiarata sia locale a un kernel sia globale in un file sorgente.
E' dichiarata con il qualificatore \texttt{\_\_shared\_\_}.
Puo' essere dichiarata sia staticamente sia dinamicamente.

\subsection{Allocazione dinamica della SMEM}
Se la dimensione non e' nota a tempo di compilazione e' possibile dichiarare una variabile adimensionale con la keyword extern:
\begin{itemize}
    \item Dinamicamente si possono allocare solo array 1D
    \item puo' essere sia internal al kernel sia esterna
\end{itemize}

\subsection{Uso tipico}
\begin{itemize}
    \item Caricare i dati dalla device memory alla shared memory
    \item Sincronizzare i thread del blocco al termine della copia che ognuno effettua sulla shared memory (cosi' che ogni thread possa elaborare dati certi nel prosieguo)
    \item Elabora i dati in shared memory
    \item Sincronizza per essere certi che la shared memory contenga i risultati aggiornati
    \item Scrivi i risultati dalla device memory alla host memory
\end{itemize}

\subsection{Prodotto matriciale con SMEM}
Nella versione senza memoria shared ogni thread e' responsabile del calcolo di un elemento della matrice prodotto. Dobbiamo tenere conto che abbiamo una suddivisione logica dei thread che stanno in un blocco e degli elementi in memoria corrispondenti.
Passi con shared memory:
\begin{itemize}
    \item Occorre caricare in memoria shared i dati relativi ad ogni blocco prima di svolgere le somme e i prodotti
    \item Ogni thread accumula i risultati di ognuno di questi prodotti (scandendo tutti i blocchi) in un registro
    \item Alla fine scrive su global memory
\end{itemize}

Qui l'idea e' di trasferire gli elementi di riga e colonna delle due matrici in shared memory perche' dobbiamo accedere ad ogni elemento tante volte e quindi l'accesso a quella memoria sarebbe piu' veloce.


L'idea e':
\begin{itemize}
    \item Caricare il primo tile 
    \item Effettuare le somme parziali per ogni cella
    \item Riptere per il numero di tile contenuti nelle zone delle matrici da caricare
    \item Scrivere i risultati dalla shared memory alla global memory
\end{itemize}

\subsection{Prodotto convolutivo con SMEM}
La convoluzione e' un prodotto tra due vettori, tendenzialmente un vettore piu' grande ed uno piu' piccolo chiamato kernel o maschera.

Una volta che abbiamo la scrittura matematica l'istanziazione e' con vettori, nell'esempio vediamo il segnale e la maschera, viene evidenziato il segnale centrale perche' e' quello in output della convoluzione:
%metti immagine implementazione
Occorre fare shiftare la maschera con uno stride = 1 e calcolare prodotti e somme.

Da notare che ci sono casi particolari, nell'esempio partiamo dal terzo elemento perche' c'e' corrispondenza reale con la maschera, si potrebbe anche adottare approcci diversi relativamente a dati mancanti.

Questa cosa si estende naturalmente a piu' dimensioni, questo e' l'esempio del 2D:
%esempio immagini 2D

Ha senso pensare all'uso della shared memory perche' alcune celle della matrice di input vengono riutilizzate più volte durante il calcolo della convoluzione (tutte quelle sotto la maschera sicuramente).

Questo e' un problema ideale per il parallel computing:
\begin{itemize}
    \item Determinare la mappa tra thread ed elementi di output
    \item Semplice approccio nei casi 1D e 2D e' organizzare i thread in grid corrispondenti in modo tale che ogni thread calcoli un elemento della matrice di convoluzione 
    \item Problema: non soddisfa per inefficienza negli accessi alla global memory
\end{itemize}

%esempio convoluzione parallela

\subsubsection{Tiling}
Divido i dati in blocchi (ad esempio, 16 elementi in 4 blocchi da 4 thread), ogni thread nel blocco fa un prodotto della convoluzione. Per ridurre l'accesso alla global memory, in cache/memoria condivisa si tengono i dati a cui l'accesso è fatto più frequentemente, ovvero i valori del "blocco di dati" assegnato al block (tutti i thread devono calcolare sullo stesso insieme di dati, o quasi), tenendo conto della dimensione della maschera (serve avere i dati "adiacenti" al blocco, quelli che sbordano (sarebbe molto utile un'immagine, si capirebbe subito)).

I dati da caricare in smem sono più dei thread nel blocco ("alone" che va al di fuori del blocco di dati stesso); in smem carico tutti i possibili dati a cui il blocco deve fare l'accesso. 

Chi carica che dati in memoria? I dati esterni al blocco potrebbero essere anche più del blocco stesso (maschera "grossa"). La soluzione è dare un ordine ai thread e dividere il più equamente possibile i caricamenti in memoria tra i thread del blocco. Si può fare facendo un tiling dei dati da caricare: il blocco viene "ripetuto" sui dati da caricare, secondo un ordine dei thread.
